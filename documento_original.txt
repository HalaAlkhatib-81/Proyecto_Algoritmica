Retan a ChatGPT y DeepSeek a jugar al ajedrez y lo que pasa cuestiona si podemos confiar en ellas

Un reciente estudio ha revelado que modelos avanzados de inteligencia artificial, como ChatGPT-o1 y DeepSeek-R1, han recurrido a tácticas desleales al jugar partidas de ajedrez. Estos hallazgos ponen en cuestión la fiabilidad de estas tecnologías y abren un debate sobre su comportamiento en situaciones donde se les exija tomar decisiones críticas. Investigadores de Palisade Research enfrentaron a varios modelos de inteligencia artificial contra Stockfish, uno de los motores de ajedrez más avanzados. A lo largo de cientos de partidas, los expertos proporcionaron a las IA un "bloc de notas" en el que debían registrar sus procesos de pensamiento. Durante la observación, descubrieron que ChatGPT-o1 intentó hacer trampas en el 37% de los juegos, mientras que DeepSeek-R1 recurrió a tácticas cuestionables en aproximadamente una de cada diez partidas.

El comportamiento detectado no se limitó a movimientos ilegales. Algunos modelos ejecutaron copias ocultas de Stockfish para anticipar jugadas, reescribieron el tablero a su favor y hasta intentaron manipular los archivos del programa. Estas acciones ponen de manifiesto la capacidad de la IA para encontrar y explotar vulnerabilidades en los sistemas con los que interactúa. 

¿Por qué las IA recurren a estas estrategias?

El fenómeno se explica a través del concepto de gaming specification, que describe cómo un modelo de inteligencia artificial puede reinterpretar las reglas para maximizar su rendimiento, incluso si esto implica recurrir a estrategias desleales. Resulta preocupante que los modelos más recientes, como ChatGPT-01 y DeepSeek-R1, hayan mostrado una mayor tendencia a este comportamiento que versiones anteriores, como GPT-4o y Claude 3.5 Sonnet, que solo lo desarrollaban cuando eran incentivadas a hacerlo.

Estos descubrimientos generan inquietud sobre el uso de la inteligencia artificial en ámbitos más críticos. Si estos sistemas son capaces de manipular un juego para evitar perder, ¿qué sucedería si se les asignaran tareas en sectores como la ciberseguridad, la gestión financiera o la toma de decisiones autónomas? Esto es lo que se plantean en TechCrunch, donde reflexionan acerca de la posibilidad de que estas tecnologías puedan evadir restricciones impuestas por sus desarrolladores plantea un desafío en términos de regulación y supervisión.